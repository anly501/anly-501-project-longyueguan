<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.1/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://getbootstrap.com/docs/5.2/assets/css/docs.css" rel="stylesheet">
    <title>Clustering</title>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.1/dist/js/bootstrap.bundle.min.js"></script>
  </head>

<style>
   .container-fluid a {

  color: #ffffff;
  
}

</style>

<nav class="navbar navbar-expand-lg bg-dark">
   
    <div class="container-fluid">
      <a class="navbar-brand" href="http://longyueguan.georgetown.domains/501-project-website/index.html">About Me</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarScroll" aria-controls="navbarScroll" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarScroll">
        <ul class="nav justify-content-center" style="--bs-scroll-height: 100px;">
          <li class="nav-item">
            <a class="nav-link active" aria-current="page" href="https://github.com/anly501/anly-501-project-longyueguan/tree/main/codes/01-data-gathering">Code</a>
          </li>
          <li class = "nav-item">
            <a class = "nav-link" href = "http://longyueguan.georgetown.domains/501-project-website/Nav.html#"> Introduction</a>
          </li>
          <li class = "nav-item">
            <a class = "nav-link" href = "http://longyueguan.georgetown.domains/501-project-website/Data_Gathering.html#"> Data Gathering</a>
          </li>
          <li class = "nav-item">
            <a class = "nav-link" href = "http://longyueguan.georgetown.domains/501-project-website/Data_Cleaning.html#"> Data Cleaning</a>
          </li>
          <li class = "nav-item">
            <a class = "nav-link" href = "http://longyueguan.georgetown.domains/501-project-website/Exploring_data.html#"> Exploring Data</a>
          </li>
          <li class = "nav-item">
            <a class = "nav-link" href = "http://longyueguan.georgetown.domains/501-project-website/Naive_Bayes.html#"> Naive Bayes</a>
          </li>
          <li class = "nav-item">
            <a class = "nav-link" href = "http://longyueguan.georgetown.domains/501-project-website/Decision_Tree.html#"> Decision Trees</a>
          </li>
          <li class = "nav-item">
            <a class = "nav-link" href = "http://longyueguan.georgetown.domains/501-project-website/SVM.html#"> SVM</a>
          </li>
          <li class = "nav-item">
            <a class = "nav-link" href = "http://longyueguan.georgetown.domains/501-project-website/Clustering.html#"> Clustering</a>
          </li>
          <li class = "nav-item">
            <a class = "nav-link" href = "http://longyueguan.georgetown.domains/501-project-website/ARM.html#"> ARM and Networking</a>
          </li>
          <li class = "nav-item">
            <a class = "nav-link" href = "http://longyueguan.georgetown.domains/501-project-website/Conclusion.html#"> Conclusion</a>
          </li>
          </ul>
      
      </div>
    </div>
  </nav>

<br>
<br>
<h3><b><i>Clustering: </h3><h5>Clustering is separating different parts of data based on common characteristics. And it could be considered the most
    important unsupervised learning problem. Unsupervised learning means no labels are given to the learning algorithm, leaving it on its own to find structure in its
    input. It can be a goal or a means toward an end. The purpose of a cluster is to give a collection of similar objects. However, it could also find dissimilar
    to the things in other clusters. 
</h5>
</i></b>
<br>


<p> This section provides the usage of unsupervised clustering. It shows the sentiment analysis with positive, neutral, and negative words in different clusters. 
    And it is necessary to prepare data before we start the clustering method. So, what we did to get the clustering data was to select columns that we wanted 
    to use in the clustering part. This time, we selected six columns from the previously cleaned dataset: sentiment_tokens, label, text, lemmas_back_to_text, has_negative, and has_postive columns
    . And we need to use a cat.codes function to replace categorical values with category codes. 
</p>
<p>
    Then we will use our cleaned data to start with the clustering method. Since we are doing unsupervised learning, we will not use the y labels. We did use iloc to get values for both 
    X and y data. We drop the label column for feature data X and save other columns from Dataframe as X and the label column as y. Because
    we do not use y labels, we choose to normalize the X data using the StandardScaler function. Then we will fit the data into the model with X_train and transform X_test
    with the model. Overall, the purpose we want to get from the clustering method is to find distinct groups within the dataset. 
  </p>


<br>

<h5><b>Theory:</b></h5>
<p>
    We did three methods in the overall clustering part. The first is KMeans, which used the elbow method in model selection. The elbow method will help us get each cluster's distortion and inertia.
     As we know, in the K-means algorithm, the goal we want to do is to minimize the distortion.
     <br>And distortion is the sum of the squared distances between our datasets and
     the centroid. We will also get inertia from the elbow method, showing how well our model fits. That means it should be a good model if the inertia becomes lower after all clusters.
</p>
<p>
    We also get the DBSAN for our clustering method, which is density-based cluster identification. It will help us to identify clusters from the dataset, and it is based on the high or low density.
    And that also means it will find the distance between the nearest clusters. 
    <br>
    To perform DBSCAN clustering, we could use the eps and min_samples parameters to find the optimal number. And in this 
    case, we set the eps as 0.3 and min_samples as 10.
</p>
<p>
    And the last method we used in the clustering part is hierarchical clustering. Overall, we did perform agglomerative clustering and showed the dendrogram for the linkage. Hierarchical clustering 
    means we will find the dissimilarities between datasets and display them as a dendrogram plot. We used agglomerative clustering, and the whole process will start with each cluster of the data 
    point. 
    <br>Overall, the whole process will be set from each cluster to a group closest to each other. For example, we might have separate points(clusters).
     And to perform the hierarchical clustering, it needs to combine similar clusters. And the dendrogram will quickly show the relationship betweeen them.
</p>

<br>

<table class="table">
  <thead>
    <tr>
      <th scope="col">Python Section</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href ='https://github.com/anly501/anly-501-project-longyueguan/blob/main/codes/Clustering/Clustering.ipynb'>Python Code for Clustering</a></td>
    </tr>
    <tr>
        <td><a href ='https://github.com/anly501/anly-501-project-longyueguan/blob/main/data/cluster_py.csv'>Data for Clustering</a></td>
      </tr>
  </tbody>
</table>
<br>


<h5><b>Results:</b></h5>
<p>
    The following two images show the result from KMeans. The first plot displays the distortion and inertia for K-means clustering using the elbow method. 
    The plot could clearly show distortion and inertia results with different clusters, and we could find out that when the cluster is increasing, both distortion 
    and inertia also decrease. And that could also mean it is a good model. And the second plot shows how the clusters display between the dataset's sentiment words
     and the original words from a dataset with the label. And label means positive, neutral, and negative words to 0, 1, and 2. And the right side plot from the 
    second one shows the original text and sentiment words with whether it has negative words. We could notice it has three clusters on the left side plot and two 
    clusters on the right side plot.
</p>
<img src="images/cluster1.png" width="450" height="300">
<br>
<img src="images/cluster2.png" width="850" height="400">
<br>
<br>

<p>
    Below, this shows the plot of DBSCAN and predicts the labels. As we mentioned, we set up the eps as 0.3 and the min samples as 10. As we know, eps is the measurement of the neighbor, 
    and min sample means the minimum number of samples in the neighborhood. By using DBSCAN, it is not necessary to put a detailed number of clusters, and it allows for noise. 
    And in the displayed plot of DBSCAN, it could find out that the yellow point should be the noise point. And others should include the core point and border point. 
    And got the Silhouette Coefficient from DBSCAN is 0.149, which is between -1 and 1. However, it is closer to -1, which is the worse score possible. 
  
</p>

<img src="images/cluster4.png" width="450" height="300">
<br>
<br>

<p>Hierarchical Clustering showed below, which is used for the agglomerative method. And we use Dendrogram to show the hierarchical clustering. It helps us easily to read and understand.
    It shows different colors and could represent they could be into other groups of clusters. Since it shows the plot with data points and cluster distance, we could find out 
    if they could combine clusters. That means they are in the same color and should be the same cluster, and the vertical position shows the dissimilarities between the two clusters.
    
</p>
<img src="images/cluster3.png" width="550" height="400">

<br>
<br>




<br>
<br>
<h5><b>Conclusion:</b></h5>
<p>
    To conclude, we used three clustering methods of our dataset and saw how three of them work. According to our results, we could find out that KMeans works well and represents a good model.
    And based on its result of distortion and inertia score, it also shows us it is a good model. Moreover, it also offers three clusters separated with positive, neutral, and negative labels
    from both the original text data and the sentiment words data. Also, it splits into two clusters of negative words or not have negative terms with the relationship between the original tweets
    text and the sentiment words.
</p>
<p>
    Lastly, we also work with both DBSCAN and hierarchical clustering. It presents a better result with hierarchical clustering than the DBSCAN. Since we got the silhouette_score from both DBSCAN 
    and hierarchical clustering methods. We could compare the results of silhouette_score of both methods. So, it shows 0.149 for the DBSCAN method and 0.274 for the hierarchical method. Since the 
    silhouette_score is between -1 to 1, and closer to 1 means a better result. We could find out that the hierarchical clustering method works better. 
</p>


<br>
<br>
<h7><b>Reference:</b></h7>
<p>
    Mahendru, Khyati. “How to Determine the Optimal K for K-Means?” Medium, Analytics Vidhya, 17 June 2019, https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb. 
</p>
<p>
    Sharma, Pranshu. “K Means Clustering Simplified in Python: K Means Algorithm.” Analytics Vidhya, 20 June 2022, https://www.analyticsvidhya.com/blog/2021/04/k-means-clustering-simplified-in-python/. 
</p>
</html>

